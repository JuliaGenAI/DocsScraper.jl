"""
    report_artifact(fn_output)

Print artifact information
"""
function report_artifact(
        fn_output, knowledge_pack_path::AbstractString, index_name::AbstractString)
    artifact = basename(fn_output)
    sha = bytes2hex(open(sha256, fn_output))
    git_tree_sha = Tar.tree_hash(IOBuffer(inflate_gzip(fn_output)))

    content = """
        ARTIFACT: "$(artifact)"
        sha256: "$(sha)"
        git-tree-sha1: "$(git_tree_sha)"
        """
    open(joinpath(knowledge_pack_path, "Index", "$(index_name)__artifact__info.txt"),
        "w") do file
        write(file, content)
    end

    @info("ARTIFACT: $(artifact)")
    @info("sha256: ", sha)
    @info("git-tree-sha1: ", git_tree_sha)
end

"""
    create_output_dirs(parent_directory_path::String, index_name::String)

Create index_name, Scraped_files and Index directories inside `parent_directory_path`. Return path to `index_name` 
"""
function create_output_dirs(parent_directory_path::String, index_name::String)
    mkpath(joinpath(parent_directory_path, index_name))
    mkpath(joinpath(parent_directory_path, index_name, "Scraped_files"))
    mkpath(joinpath(parent_directory_path, index_name, "Index"))
    return joinpath(parent_directory_path, index_name)
end

"""
    make_chunks(hostname_url_dict::Dict{AbstractString,Vector{AbstractString}}, knowledge_pack_path::String; 
        max_chunk_size::Int=MAX_CHUNK_SIZE, min_chunk_size::Int=MIN_CHUNK_SIZE)

Parse URLs from hostname_url_dict and save the chunks

# Arguments
- hostname_url_dict: Dict with key being hostname and value being a vector of URLs
- knowledge_pack_path: Knowledge pack path
- max_chunk_size: Maximum chunk size
- min_chunk_size: Minimum chunk size
"""
function make_chunks(hostname_url_dict::Dict{AbstractString, Vector{AbstractString}},
        knowledge_pack_path::String; max_chunk_size::Int = MAX_CHUNK_SIZE,
        min_chunk_size::Int = MIN_CHUNK_SIZE)
    scraped_files_dir = joinpath(knowledge_pack_path, "Scraped_files")
    for (hostname, urls) in hostname_url_dict
        output_chunks = Vector{SubString{String}}()
        output_sources = Vector{String}()
        for url in urls
            try
                chunks, sources = process_paths(
                    url; max_chunk_size, min_chunk_size)
                append!(output_chunks, chunks)
                append!(output_sources, sources)
            catch
                @error "error!! check url: $url"
            end
        end
        serialize(
            joinpath(scraped_files_dir,
                "$(hostname)-chunks-max-$(max_chunk_size)-min-$(min_chunk_size).jls"),
            output_chunks)
        serialize(
            joinpath(scraped_files_dir,
                "$(hostname)-sources-max-$(max_chunk_size)-min-$(min_chunk_size).jls"),
            output_sources)
    end
end

"""
    l2_norm_columns(mat::AbstractMatrix)

Normalize the columns of the input embeddings
"""
function l2_norm_columns(mat::AbstractMatrix)
    norm_ = norm.(eachcol(mat))
    return mat ./ norm_'
end

"""
    l2_norm_columns(vect::AbstractVector)

Normalize the columns of the input embeddings
"""
function l2_norm_columns(vect::AbstractVector)
    norm_ = norm(vect)
    return vect / norm_
end

"""
    remove_dashes(text::AbstractString)

removes all dashes ('-') from a given string
"""
function process_text(text::AbstractString)
    return replace(lowercase(text), "-" => "", "_" => "")
end

"""
    generate_embeddings(knowledge_pack_path::String; model_embedding::AbstractString=MODEL_EMBEDDING, 
        embedding_dimension::Int=EMBEDDING_DIMENSION, custom_metadata::AbstractString,
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "", save_url_map::Bool = true)

Deserialize chunks and sources to generate embeddings. Returns path to tar.gz file of the created index
Note: We recommend passing `index_name`. This will be the name of the generated index


# Arguments
- model_embedding: Embedding model
- embedding_dimension: Embedding dimensions
- custom_metadata: Custom metadata like ecosystem name if required
- embedding_bool: If true, embeddings generated will be boolean, Float32 otherwise
- index_name: Name of the index. Default: "index" symbol generated by gensym
- save_url_map: If true, creates a CSV of crawled URLs with their associated package names
"""
function generate_embeddings(
        knowledge_pack_path::String; max_chunk_size::Int = MAX_CHUNK_SIZE,
        model_embedding::AbstractString = MODEL_EMBEDDING,
        embedding_dimension::Int = EMBEDDING_DIMENSION, custom_metadata::AbstractString,
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "", save_url_map::Bool = true)
    embedder = RT.BatchEmbedder()
    scraped_files_dir = joinpath(knowledge_pack_path, "Scraped_files")
    entries = readdir(joinpath(knowledge_pack_path, scraped_files_dir))

    # Regular expressions to match the file patterns of chunks and sources
    chunks_pattern = r"^(.*)-chunks-max-(\d+)-min-(\d+)\.jls$"
    sources_pattern = r"^(.*)-sources-max-(\d+)-min-(\d+)\.jls$"

    chunks = Vector{SubString{String}}()
    sources = Vector{String}()

    for file in entries
        chunk_match = match(chunks_pattern, file)
        source_match = match(sources_pattern, file)

        if chunk_match !== nothing
            chunks_file_path = joinpath(scraped_files_dir, file)
            append!(chunks, deserialize(chunks_file_path))
        elseif source_match !== nothing
            sources_file_path = joinpath(scraped_files_dir, file)
            append!(sources, deserialize(sources_file_path))
        end
    end

    index_name = process_text(index_name)

    # Generate embeddings
    cost_tracker = Threads.Atomic{Float64}(0.0)
    full_embeddings = RT.get_embeddings(
        embedder, chunks; model_embedding, verbose = false, cost_tracker)
    @info "Created embeddings for $index_name. Cost: \$$(round(cost_tracker[], digits=3))"

    full_embeddings = full_embeddings[1:embedding_dimension, :] |>
                      l2_norm_columns
    if embedding_bool
        full_embeddings = map(>(0), full_embeddings)
    end

    trunc = embedding_dimension < EMBEDDING_DIMENSION ? 1 : 0
    emb_data_type = embedding_bool ? "Bool" : "Float32"
    date = Dates.today()
    date_string = Dates.format(Dates.today(), "yyyymmdd")

    file_name = "$(index_name)__v$(date_string)__$(process_text(model_embedding))-$(embedding_dimension)-$(emb_data_type)__v1.0"
    fn_output = joinpath(knowledge_pack_path, "Index",
        "$(file_name).tar.gz")
    fn_temp = joinpath(knowledge_pack_path, "Index",
        "$(file_name).hdf5")

    h5open(fn_temp, "w") do file
        file["chunks"] = chunks
        file["sources"] = sources
        file["embeddings"] = full_embeddings
        file["type"] = "ChunkIndex"

        package_url_dict = Dict{String, Vector{String}}()
        package_url_dict = urls_for_metadata(sources)

        metadata = Dict(
            :embedded_dt => Dates.today(),
            :custom_metadata => custom_metadata, :max_chunk_size => max_chunk_size,
            :embedding_dimension => embedding_dimension, :model_embedding => model_embedding,
            :packages => package_url_dict)

        metadata_json = JSON.json(metadata)
        file["metadata"] = metadata_json
    end

    command = `tar -cvzf $fn_output -C $(dirname(fn_temp)) $(basename(fn_temp))`
    run(command)
    report_artifact(fn_output, knowledge_pack_path, index_name)
    if save_url_map
        create_URL_map(sources, knowledge_pack_path, index_name)
    end
    return fn_output
end

#TODO: model -> model_embedding, embedding_size -> embedding_dimension, bool_embeddings to embedding_bool
"""
    make_knowledge_packs(crawlable_urls::Vector{<:AbstractString} = String[];
        single_urls::Vector{<:AbstractString} = String[],
        max_chunk_size::Int = MAX_CHUNK_SIZE, min_chunk_size::Int = MIN_CHUNK_SIZE,
        model_embedding::AbstractString = MODEL_EMBEDDING, embedding_dimension::Int = EMBEDDING_DIMENSION, custom_metadata::AbstractString = "",
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "",
        target_path::AbstractString = "", save_url_map::Bool = true)

Entry point to crawl, parse and generate embeddings. Returns path to tar.gz file of the created index
Note: We recommend passing `index_name`. This will be the name of the generated index

# Arguments
- crawlable_urls: URLs that should be crawled to find more links
- single_urls: Single page URLs that should just be scraped and parsed. The crawler won't look for more URLs
- max_chunk_size: Maximum chunk size
- min_chunk_size: Minimum chunk size
- model_embedding: Embedding model
- embedding_dimension: Embedding dimensions
- custom_metadata: Custom metadata like ecosystem name if required
- embedding_bool: If true, embeddings generated will be boolean, Float32 otherwise
- index_name: Name of the index. Default: "index" symbol generated by gensym  
- target_path: Path to the directory where the index folder will be created
- save_url_map: If true, creates a CSV of crawled URLs with their associated package names
"""
function make_knowledge_packs(crawlable_urls::Vector{<:AbstractString} = String[];
        single_urls::Vector{<:AbstractString} = String[],
        max_chunk_size::Int = MAX_CHUNK_SIZE, min_chunk_size::Int = MIN_CHUNK_SIZE,
        model_embedding::AbstractString = MODEL_EMBEDDING, embedding_dimension::Int = EMBEDDING_DIMENSION,
        custom_metadata::AbstractString = "",
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "",
        target_path::AbstractString = "", save_url_map::Bool = true)
    if isempty(crawlable_urls) && isempty(single_urls)
        error("At least one of `input_urls` or `single_pages` must be provided.")
    end

    hostname_url_dict = Dict{AbstractString, Vector{AbstractString}}()

    if !isempty(crawlable_urls)
        hostname_url_dict, visited_url_set = crawl(crawlable_urls)
    else
        visited_url_set = Set{AbstractString}()
    end
    ## TODO: seaprate into diff functs
    for url in single_urls
        base_url = get_base_url(url)
        if !in(base_url, visited_url_set)
            push!(visited_url_set, base_url)
            crawlable, sitemap_urls = check_robots_txt("*", base_url)
            if crawlable
                try
                    process_hostname!(url, hostname_url_dict)
                catch
                    @error "Bad URL: $base_url"
                end
            end
        end
    end
    if !ispath(target_path)
        @error "Target path $target_path does not exist"
        target_path = joinpath(@__DIR__, "..", "knowledge_packs")
        @info "Creating index in $target_path"
    end

    if isempty(index_name)
        index_name = "$(gensym("index"))"
    end
    target_path = create_output_dirs(target_path, index_name)

    make_chunks(
        hostname_url_dict, target_path; max_chunk_size, min_chunk_size)
    ## TODO: seaprate into diff functs
    index = generate_embeddings(
        target_path; max_chunk_size, model_embedding, embedding_dimension,
        custom_metadata, embedding_bool, index_name, save_url_map)
    return index
end

## TODO: put this in utilities
# function embed_and_pack(;x
#         max_chunk_size::Int = MAX_CHUNK_SIZE, min_chunk_size::Int = MIN_CHUNK_SIZE,
#         model_embedding::AbstractString = MODEL_EMBEDDING, embedding_dimension::Int = EMBEDDING_DIMENSION, custom_metadata::AbstractString = "",
#         embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "")
#     knowledge_pack_path = joinpath(@__DIR__, "..", "knowledge_packs")
#     generate_embeddings(
#         knowledge_pack_path; max_chunk_size, model_embedding, embedding_dimension,
#         custom_metadata, embedding_bool, index_name)
# end