"""
    report_artifact(fn_output)

Print artifact information
"""
function report_artifact(
        fn_output, target_path::AbstractString, index_name::AbstractString)
    artifact = basename(fn_output)
    sha = bytes2hex(open(sha256, fn_output))
    git_tree_sha = Tar.tree_hash(IOBuffer(inflate_gzip(fn_output)))

    content = """
        ARTIFACT: "$(artifact)"
        sha256: "$(sha)"
        git-tree-sha1: "$(git_tree_sha)"
        """
    open(joinpath(target_path, "Index", "$(index_name)__artifact__info.txt"),
        "w") do file
        write(file, content)
    end

    @info("ARTIFACT: $(artifact)")
    @info("sha256: ", sha)
    @info("git-tree-sha1: ", git_tree_sha)
end

"""
    create_output_dirs(parent_directory_path::String, index_name::String)

Create index_name, Scraped_files and Index directories inside `parent_directory_path`. Return path to `index_name` 
"""
function create_output_dirs(parent_directory_path::String, index_name::String)
    mkpath(joinpath(parent_directory_path, index_name))
    mkpath(joinpath(parent_directory_path, index_name, "Scraped_files"))
    mkpath(joinpath(parent_directory_path, index_name, "Index"))
    return joinpath(parent_directory_path, index_name)
end

"""
    l2_norm_columns(mat::AbstractMatrix)

Normalize the columns of the input embeddings
"""
function l2_norm_columns(mat::AbstractMatrix)
    norm_ = norm.(eachcol(mat))
    return mat ./ norm_'
end

"""
    l2_norm_columns(vect::AbstractVector)

Normalize the columns of the input embeddings
"""
function l2_norm_columns(vect::AbstractVector)
    norm_ = norm(vect)
    return vect / norm_
end

"""
    remove_dashes(text::AbstractString)

removes all dashes ('-') from a given string
"""
function process_text(text::AbstractString)
    return replace(lowercase(text), "-" => "", "_" => "")
end

"""
    validate_args(crawlable_urls::Vector{<:AbstractString} = String[];
        single_urls::Vector{<:AbstractString} = String[], target_path::AbstractString = "", index_name::AbstractString = "")

Validate args. Return error if both `crawlable_urls` and `single_urls` are empty. 
Create a target path if input path is invalid. Create a gensym index if the input index is inavlid. 

# Arguments
- crawlable_urls: URLs that should be crawled to find more links
- single_urls: Single page URLs that should just be scraped and parsed. The crawler won't look for more URLs
- target_path: Path to the directory where the index folder will be created
- index_name: Name of the index. Default: "index" symbol generated by gensym  
"""
function validate_args(crawlable_urls::Vector{<:AbstractString} = String[];
        single_urls::Vector{<:AbstractString} = String[], target_path::AbstractString = "", index_name::AbstractString = "")
    if isempty(crawlable_urls) && isempty(single_urls)
        error("At least one of `input_urls` or `single_pages` must be provided.")
    end
    if !ispath(target_path)
        @error "Target path $target_path does not exist"
        target_path = joinpath(@__DIR__, "..", "knowledge_packs")
        @info "Index path is set to: $target_path"
    end

    index_name = process_text(index_name)

    if isempty(index_name)
        index_name = "$(gensym("index"))"
    end

    return target_path, index_name
end

"""
    process_non_crawl_urls(
        single_urls::Vector{<:AbstractString}, visited_url_set::Set{AbstractString},
        hostname_url_dict::Dict{AbstractString, Vector{AbstractString}})

Check if the `single_urls` is scrapable. If yes, then add it to a Dict of URLs to scrape 

# Arguments
- single_urls: Single page URLs that should just be scraped and parsed. The crawler won't look for more URLs
- visited_url_set: Set of visited URLs. Avoids duplication
- hostname_url_dict: Dict with key being the hostname and the values being the URLs
"""
function process_non_crawl_urls(
        single_urls::Vector{<:AbstractString}, visited_url_set::Set{AbstractString},
        hostname_url_dict::Dict{AbstractString, Vector{AbstractString}})
    for url in single_urls
        base_url = get_base_url(url)
        if !in(base_url, visited_url_set)
            push!(visited_url_set, base_url)
            crawlable, sitemap_urls = check_robots_txt("*", base_url)
            if crawlable
                try
                    process_hostname!(url, hostname_url_dict)
                catch
                    @error "Bad URL: $base_url"
                end
            end
        end
    end
end

"""
    make_chunks(hostname_url_dict::Dict{AbstractString,Vector{AbstractString}}, target_path::String; 
        max_chunk_size::Int=MAX_CHUNK_SIZE, min_chunk_size::Int=MIN_CHUNK_SIZE)

Parse URLs from hostname_url_dict and save the chunks

# Arguments
- hostname_url_dict: Dict with key being hostname and value being a vector of URLs
- target_path: Knowledge pack path
- max_chunk_size: Maximum chunk size
- min_chunk_size: Minimum chunk size
"""
function make_chunks_sources(
        hostname_url_dict::Dict{AbstractString, Vector{AbstractString}},
        target_path::String; max_chunk_size::Int = MAX_CHUNK_SIZE,
        min_chunk_size::Int = MIN_CHUNK_SIZE)
    scraped_files_dir = joinpath(target_path, "Scraped_files")
    for (hostname, urls) in hostname_url_dict
        output_chunks = Vector{SubString{String}}()
        output_sources = Vector{String}()
        for url in urls
            try
                chunks, sources = process_paths(
                    url; max_chunk_size, min_chunk_size)
                append!(output_chunks, chunks)
                append!(output_sources, sources)
            catch
                @error "error!! check url: $url"
            end
        end
        serialize(
            joinpath(scraped_files_dir,
                "$(hostname)-chunks-max-$(max_chunk_size)-min-$(min_chunk_size).jls"),
            output_chunks)
        serialize(
            joinpath(scraped_files_dir,
                "$(hostname)-sources-max-$(max_chunk_size)-min-$(min_chunk_size).jls"),
            output_sources)
    end
end

"""
    load_chunks_sources(target_path::AbstractString)

Return chunks, sources by reading the .jls files in `joinpath(target_path, "Scraped_files")` 
"""
function load_chunks_sources(target_path::AbstractString)
    scraped_files_dir = joinpath(target_path, "Scraped_files")
    entries = readdir(joinpath(target_path, scraped_files_dir))

    # Regular expressions to match the file patterns of chunks and sources
    chunks_pattern = r"^(.*)-chunks-max-(\d+)-min-(\d+)\.jls$"
    sources_pattern = r"^(.*)-sources-max-(\d+)-min-(\d+)\.jls$"

    chunks = Vector{SubString{String}}()
    sources = Vector{String}()

    for file in entries
        chunk_match = match(chunks_pattern, file)
        source_match = match(sources_pattern, file)

        if chunk_match !== nothing
            chunks_file_path = joinpath(scraped_files_dir, file)
            append!(chunks, deserialize(chunks_file_path))
        elseif source_match !== nothing
            sources_file_path = joinpath(scraped_files_dir, file)
            append!(sources, deserialize(sources_file_path))
        end
    end
    return chunks, sources
end

"""
    generate_embeddings(target_path::String; model_embedding::AbstractString=MODEL_EMBEDDING, 
        embedding_dimension::Int=EMBEDDING_DIMENSION, custom_metadata::AbstractString,
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "", save_url_map::Bool = true)

Deserialize chunks and sources to generate embeddings. Returns path to tar.gz file of the created index
Note: We recommend passing `index_name`. This will be the name of the generated index


# Arguments
- model_embedding: Embedding model
- embedding_dimension: Embedding dimensions
- custom_metadata: Custom metadata like ecosystem name if required
- embedding_bool: If true, embeddings generated will be boolean, Float32 otherwise
- index_name: Name of the index. Default: "index" symbol generated by gensym
- save_url_map: If true, creates a CSV of crawled URLs with their associated package names
"""
function generate_embeddings(
        chunks::AbstractVector{<:AbstractString}; model_embedding::AbstractString = MODEL_EMBEDDING,
        embedding_dimension::Int = EMBEDDING_DIMENSION, embedding_bool::Bool = EMBEDDING_BOOL,
        index_name::AbstractString = "")
    embedder = RT.BatchEmbedder()

    # Generate embeddings
    cost_tracker = Threads.Atomic{Float64}(0.0)
    full_embeddings = RT.get_embeddings(
        embedder, chunks; model_embedding, verbose = false, cost_tracker)
    @info "Created embeddings for $index_name. Cost: \$$(round(cost_tracker[], digits=3))"

    full_embeddings = full_embeddings[1:embedding_dimension, :] |>
                      l2_norm_columns
    if embedding_bool
        full_embeddings = map(>(0), full_embeddings)
    end

    return full_embeddings
end

"""
    save_embeddings(index_name::AbstractString, embedding_dimension::Int,
        embedding_bool::Bool, model_embedding::AbstractString, target_path::AbstractString,
        chunks::AbstractVector{<:AbstractString}, sources::Vector{String},
        full_embeddings, custom_metadata::AbstractString, max_chunk_size::Int)
    
Save the generated embeddings along with a .txt containing the artifact info

# Arguments
- index_name: Name of the index. Default: "index" symbol generated by gensym  
- embedding_dimension: Embedding dimensions
- target_path: Path to the index folder
- chunks: List of scraped chunks
- sources: List of scraped sources
- full_embeddings: Generated embedding matrix
- max_chunk_size: Maximum chunk size
"""
function save_embeddings(index_name::AbstractString, embedding_dimension::Int,
        embedding_bool::Bool, model_embedding::AbstractString, target_path::AbstractString,
        chunks::AbstractVector{<:AbstractString}, sources::Vector{String},
        full_embeddings, custom_metadata::AbstractString, max_chunk_size::Int)
    trunc = embedding_dimension < EMBEDDING_DIMENSION ? 1 : 0
    emb_data_type = embedding_bool ? "Bool" : "Float32"
    date = Dates.today()
    date_string = Dates.format(Dates.today(), "yyyymmdd")

    file_name = "$(index_name)__v$(date_string)__$(process_text(model_embedding))-$(embedding_dimension)-$(emb_data_type)__v1.0"
    fn_output = joinpath(target_path, "Index",
        "$(file_name).tar.gz")
    fn_temp = joinpath(target_path, "Index",
        "$(file_name).hdf5")

    h5open(fn_temp, "w") do file
        file["chunks"] = chunks
        file["sources"] = sources
        file["embeddings"] = full_embeddings
        file["type"] = "ChunkIndex"

        package_url_dict = Dict{String, Vector{String}}()
        package_url_dict = urls_for_metadata(sources)

        metadata = Dict(
            :embedded_dt => Dates.today(),
            :custom_metadata => custom_metadata, :max_chunk_size => max_chunk_size,
            :embedding_dimension => embedding_dimension, :model_embedding => model_embedding,
            :packages => package_url_dict)

        metadata_json = JSON.json(metadata)
        file["metadata"] = metadata_json
    end

    command = `tar -cvzf $fn_output -C $(dirname(fn_temp)) $(basename(fn_temp))`
    run(command)
    report_artifact(fn_output, target_path, index_name)
    return fn_output
end

"""
    make_knowledge_packs(crawlable_urls::Vector{<:AbstractString} = String[];
        single_urls::Vector{<:AbstractString} = String[],
        max_chunk_size::Int = MAX_CHUNK_SIZE, min_chunk_size::Int = MIN_CHUNK_SIZE,
        model_embedding::AbstractString = MODEL_EMBEDDING, embedding_dimension::Int = EMBEDDING_DIMENSION, custom_metadata::AbstractString = "",
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "",
        target_path::AbstractString = "", save_url_map::Bool = true)

Entry point to crawl, parse and generate embeddings. Returns path to tar.gz file of the created index
Note: We recommend passing `index_name`. This will be the name of the generated index

# Arguments
- crawlable_urls: URLs that should be crawled to find more links
- single_urls: Single page URLs that should just be scraped and parsed. The crawler won't look for more URLs
- max_chunk_size: Maximum chunk size
- min_chunk_size: Minimum chunk size
- model_embedding: Embedding model
- embedding_dimension: Embedding dimensions
- custom_metadata: Custom metadata like ecosystem name if required
- embedding_bool: If true, embeddings generated will be boolean, Float32 otherwise
- index_name: Name of the index. Default: "index" symbol generated by gensym  
- target_path: Path to the directory where the index folder will be created
- save_url_map: If true, creates a CSV of crawled URLs with their associated package names
"""
function make_knowledge_packs(crawlable_urls::Vector{<:AbstractString} = String[];
        single_urls::Vector{<:AbstractString} = String[],
        max_chunk_size::Int = MAX_CHUNK_SIZE, min_chunk_size::Int = MIN_CHUNK_SIZE,
        model_embedding::AbstractString = MODEL_EMBEDDING, embedding_dimension::Int = EMBEDDING_DIMENSION,
        custom_metadata::AbstractString = "",
        embedding_bool::Bool = EMBEDDING_BOOL, index_name::AbstractString = "",
        target_path::AbstractString = "", save_url_map::Bool = true)

    # Validate the input args
    target_path, index_name = validate_args(
        crawlable_urls; single_urls, target_path, index_name)

    # Crawl the crawlable urls
    hostname_url_dict, visited_url_set = crawl(crawlable_urls)

    # See if non crawlable urls are scrapable
    process_non_crawl_urls(single_urls, visited_url_set, hostname_url_dict)

    # Create output directories to save the scraped files and index
    target_path = create_output_dirs(target_path, index_name)

    # Scrape URLs and make chunks
    make_chunks_sources(
        hostname_url_dict, target_path; max_chunk_size, min_chunk_size)

    # load chunks and sources to generate embeddings
    chunks, sources = load_chunks_sources(target_path)

    # Generate embeddings
    embeddings = generate_embeddings(
        chunks; model_embedding, embedding_dimension, embedding_bool, index_name)

    # save the generated embeddings
    index = save_embeddings(
        index_name, embedding_dimension, embedding_bool, model_embedding,
        target_path, chunks, sources, embeddings, custom_metadata, max_chunk_size)

    # Create a csv of url and it's package_name
    if save_url_map
        create_URL_map(sources, target_path, index_name)
    end

    return index
end
